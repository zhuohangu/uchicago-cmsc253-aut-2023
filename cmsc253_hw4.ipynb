{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la # matrix rank, inverse import scipy.io\n",
    "import matplotlib.pyplot as plt # plots\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 Gram-Schmidt. <br>\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.13304645e-01, -5.43794290e-01,  1.99095308e-01,\n",
       "        -5.51132525e-02,  1.19578493e-02, -1.96870730e-03,\n",
       "        -2.10219152e-04],\n",
       "       [ 4.06652323e-01,  3.03317262e-01, -6.88560548e-01,\n",
       "         4.75965265e-01, -1.97392654e-01,  5.41151990e-02,\n",
       "         3.01288446e-03],\n",
       "       [ 2.71101548e-01,  3.93949644e-01, -2.07134626e-01,\n",
       "        -4.90111167e-01,  6.10839674e-01, -3.26893986e-01,\n",
       "         1.54577469e-02],\n",
       "       [ 2.03326161e-01,  3.81744394e-01,  1.12389156e-01,\n",
       "        -4.39598473e-01, -2.54179063e-01,  6.41045574e-01,\n",
       "        -2.08187091e-01],\n",
       "       [ 1.62660929e-01,  3.51412668e-01,  2.91518455e-01,\n",
       "        -1.12276608e-01, -4.99206689e-01, -2.02224426e-01,\n",
       "         6.15118567e-01],\n",
       "       [ 1.35550774e-01,  3.20235052e-01,  3.89191824e-01,\n",
       "         2.30886766e-01, -1.50594721e-01, -5.41817789e-01,\n",
       "        -7.06123659e-01],\n",
       "       [ 1.16186378e-01,  2.92095792e-01,  4.40744903e-01,\n",
       "         5.20623748e-01,  5.01273338e-01,  3.80535581e-01,\n",
       "         2.81830797e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gram_schmidt(X):\n",
    "    # X is an n-by-p matrix.\n",
    "    # Returns U an orthonormal matrix.\n",
    "    # eps is a threshold value to identify if a vector # is nearly a zero vector.\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    n, p = X.shape\n",
    "    U = np.zeros((n, 0))\n",
    "    for j in range(p):\n",
    "        # Get the j-th column of matrix X\n",
    "        v = X[:, j]\n",
    "        # Write your own code here: Perform the\n",
    "        # orthogonalization by subtracting the projections on\n",
    "        # all columns of U. And then check whether the vector\n",
    "        # you get is nearly a zero vector.\n",
    "        v = v - U@U.T@v\n",
    "        v = np.reshape(v, (-1, 1))        \n",
    "        if np.linalg.norm(v) > eps:\n",
    "            # Normalize vector v and append it to U\n",
    "            U = np.hstack((U, v / la.norm(v)))\n",
    "    return U\n",
    "X = np.array([[1, 1/2, 1/3, 1/4, 1/5, 1/6, 1/7],\n",
    "              [1/2, 1/3, 1/4, 1/5, 1/6, 1/7, 1/8],\n",
    "              [1/3, 1/4, 1/5, 1/6, 1/7, 1/8, 1/9],\n",
    "              [1/4, 1/5, 1/6, 1/7, 1/8, 1/9, 1/10],\n",
    "              [1/5, 1/6, 1/7, 1/8, 1/9, 1/10, 1/11],\n",
    "              [1/6, 1/7, 1/8, 1/9, 1/10, 1/11, 1/12],\n",
    "              [1/7, 1/8, 1/9, 1/10, 1/11, 1/12, 1/13]])\n",
    "gram_schmidt(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  6.73072709e-16 -2.36616282e-15  2.80331314e-14\n",
      "  -1.92849903e-12  1.25247715e-10 -9.71088800e-09]\n",
      " [ 6.73072709e-16  0.00000000e+00 -2.76723089e-14  3.71008779e-13\n",
      "  -2.63988831e-12 -4.05814549e-11  5.49175827e-09]\n",
      " [-2.36616282e-15 -2.76723089e-14  1.11022302e-16  8.52865001e-12\n",
      "  -2.31620473e-10  5.20528393e-09 -1.33284293e-07]\n",
      " [ 2.80331314e-14  3.71008779e-13  8.52865001e-12 -2.22044605e-16\n",
      "  -9.04092928e-09  4.29613409e-07 -1.80981921e-05]\n",
      " [-1.92849903e-12 -2.63988831e-12 -2.31620473e-10 -9.04092928e-09\n",
      "   0.00000000e+00  2.66371580e-05 -2.30322707e-03]\n",
      " [ 1.25247715e-10 -4.05814549e-11  5.20528393e-09  4.29613409e-07\n",
      "   2.66371580e-05  0.00000000e+00 -2.27098006e-01]\n",
      " [-9.71088800e-09  5.49175827e-09 -1.33284293e-07 -1.80981921e-05\n",
      "  -2.30322707e-03 -2.27098006e-01  1.11022302e-16]]\n",
      "L1 matrix norm is 0.22941947980869065\n"
     ]
    }
   ],
   "source": [
    "def hilbert_matrix(n):\n",
    "    X = np.array([[1.0 / (i + j - 1) for i in \\\n",
    "                   range(1, n + 1)] for j in range(1, n + 1)])\n",
    "    return X\n",
    "n = 7\n",
    "X = hilbert_matrix(n)\n",
    "U = gram_schmidt(X)\n",
    "L1 = np.identity((U.T@U).shape[0]) - U.T@U\n",
    "print(L1)\n",
    "err = la.norm(L1, ord=1)\n",
    "print(f'L1 matrix norm is {err}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.54737334e-15 -2.16146545e-14  3.62931907e-13\n",
      "  -7.97459321e-12  3.16995839e-10 -1.90386089e-08]\n",
      " [ 1.54737334e-15  0.00000000e+00  1.02695630e-15 -2.63955524e-14\n",
      "   9.00696184e-13 -2.74882339e-11  1.03917761e-09]\n",
      " [-2.16146545e-14  1.02695630e-15 -2.22044605e-16  3.05311332e-15\n",
      "  -2.71893619e-13  1.51737511e-11 -6.93317848e-10]\n",
      " [ 3.62931907e-13 -2.63955524e-14  3.05311332e-15  0.00000000e+00\n",
      "  -1.66533454e-15 -2.34368080e-13  4.83647289e-11]\n",
      " [-7.97459321e-12  9.00696184e-13 -2.71893619e-13 -1.66533454e-15\n",
      "   0.00000000e+00  2.01227923e-14 -2.16494878e-12]\n",
      " [ 3.16995839e-10 -2.74882339e-11  1.51737511e-11 -2.34368080e-13\n",
      "   2.01227923e-14  2.22044605e-16 -1.46549439e-14]\n",
      " [-1.90386089e-08  1.03917761e-09 -6.93317848e-10  4.83647289e-11\n",
      "  -2.16494878e-12 -1.46549439e-14 -2.22044605e-16]]\n",
      "Modified L1 matrix norm is 2.0821648873819987e-08\n"
     ]
    }
   ],
   "source": [
    "def modified_gram_schmidt(X):\n",
    "    # Define a threshold value to identify if a vector # is nearly a zero vector.\n",
    "    eps = 1e-12\n",
    "    n, p = X.shape\n",
    "    U = np.zeros((n, 0))\n",
    "    for j in range(p):\n",
    "        # Get the j-th column of matrix X\n",
    "        v = X[:, j]\n",
    "        for i in range(j):\n",
    "            # Compute and subtract the projection of\n",
    "            # vector v onto the i-th column of U\n",
    "            v = v - np.dot(U[:, i], v) * U[:, i]\n",
    "        v = np.reshape(v, (-1, 1))\n",
    "        # Check whether the vector we get is nearly # a zero vector\n",
    "        if np.linalg.norm(v) > eps:\n",
    "            # Normalize vector v and append it to U\n",
    "            U = np.hstack((U, v / la.norm(v)))\n",
    "    return U\n",
    "n = 7\n",
    "X = hilbert_matrix(n)\n",
    "U_modified = modified_gram_schmidt(X)\n",
    "L1_modified = np.identity((U_modified.T@U_modified).shape[0]) - \\\n",
    "    U_modified.T@U_modified\n",
    "\n",
    "L1_modified\n",
    "print(L1_modified)\n",
    "err_modified = la.norm(L1_modified, ord=1)\n",
    "print(f'Modified L1 matrix norm is {err_modified}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the traditional approach to Gram-Schmidt, each vector in the sequence is made orthogonal to all previously processed vectors by subtracting their projections. The modified Gram-Schmidt re-orthogonalizes each vector against all previously orthogonalized vectors, one at a time. This stepwise approach reduces the accumulation of errors and maintains better orthogonality. Therefore, the modified Gram-Schmidtis process is generally superior to the original process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 <br>\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error estimate: 0.052455357142857144\n"
     ]
    }
   ],
   "source": [
    "d = np.load('face_emotion_data.npz')\n",
    "X = d['X']\n",
    "y = d['y']\n",
    "n, p = np.shape(X)\n",
    "# error rate for truncated SVD\n",
    "error_SVD = np.zeros((8, 7))\n",
    "# SVD parameters to test\n",
    "k_vals = np.arange(9) + 1\n",
    "param_err_SVD = np.zeros(len(k_vals))\n",
    "\n",
    "subset_count = 8\n",
    "\n",
    "subsets = np.array_split(np.arange(n), 8)\n",
    "\n",
    "for i in range(subset_count):\n",
    "    X_hold_out = X[subsets[i]]\n",
    "    y_hold_out = y[subsets[i]]\n",
    "    \n",
    "    for idx, j in enumerate([j for j in range(subset_count) if j != i]):\n",
    "        X_train = np.concatenate([X[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        y_train = np.concatenate([y[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        \n",
    "        U, Sigma, VT = la.svd(X_train, full_matrices=False)\n",
    "\n",
    "        X_test = X[subsets[j]]\n",
    "        y_test = y[subsets[j]]\n",
    "\n",
    "        w_hat_lst = list()\n",
    "\n",
    "        for k_idx, k in enumerate(k_vals):\n",
    "            Sigma_plus = np.diag(1 / Sigma[:k])\n",
    "            VT_k = VT[:k, :]\n",
    "            U_k = U[:, :k]\n",
    "\n",
    "            w_hat = VT_k.T@Sigma_plus@U_k.T@y_train\n",
    "            w_hat_lst.append(w_hat)\n",
    "            y_hat = X_test@w_hat\n",
    "\n",
    "            param_err_SVD[k_idx] += np.sum(np.sign(y_hat) != y_test) / 16\n",
    "\n",
    "        # print(param_err_SVD)\n",
    "        k_best = np.argmin(param_err_SVD)\n",
    "        w_hat_best = w_hat_lst[k_best]\n",
    "\n",
    "        y_hat_hold_out = X_hold_out@w_hat_best\n",
    "        error_SVD[i][idx] = np.sum(np.sign(y_hat_hold_out) != y_hold_out) / 16\n",
    "print(f\"Error estimate: {np.mean(error_SVD)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error estimate: 0.0546875\n"
     ]
    }
   ],
   "source": [
    "d = np.load('face_emotion_data.npz')\n",
    "X = d['X']\n",
    "y = d['y']\n",
    "n, p = np.shape(X)\n",
    "# error rate for regularized least squares\n",
    "error_RLS = np.zeros((8, 7))\n",
    "# RLS parameters to test\n",
    "lambda_vals = np.array([0, 0.5, 1, 2, 4, 8, 16])\n",
    "param_err_RLS = np.zeros(len(lambda_vals))\n",
    "\n",
    "subset_count = 8\n",
    "\n",
    "subsets = np.array_split(np.arange(n), 8)\n",
    "\n",
    "for i in range(subset_count):\n",
    "    X_hold_out = X[subsets[i]]\n",
    "    y_hold_out = y[subsets[i]]\n",
    "    \n",
    "    for idx, j in enumerate([j for j in range(subset_count) if j != i]):\n",
    "        X_train = np.concatenate([X[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        y_train = np.concatenate([y[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        \n",
    "        X_test = X[subsets[j]]\n",
    "        y_test = y[subsets[j]]\n",
    "\n",
    "        w_hat_lst = list()\n",
    "\n",
    "        for lmd_idx, lmd in enumerate(lambda_vals):\n",
    "            w_hat = la.inv(X_train.T@X_train + lmd*np.identity(p))@X_train.T@y_train\n",
    "            w_hat_lst.append(w_hat)\n",
    "            y_hat = X_test@w_hat\n",
    "\n",
    "            param_err_RLS[lmd_idx] += np.sum(np.sign(y_hat) != y_test) / 16\n",
    "\n",
    "        # print(param_err_SVD)\n",
    "        k_best = np.argmin(param_err_RLS)\n",
    "        w_hat_best = w_hat_lst[k_best]\n",
    "\n",
    "        y_hat_hold_out = X_hold_out@w_hat_best\n",
    "        error_RLS[i][idx] = np.sum(np.sign(y_hat_hold_out) != y_hold_out) / 16\n",
    "print(f\"Error estimate: {np.mean(error_RLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error estimate: 0.049107142857142856\n"
     ]
    }
   ],
   "source": [
    "X = d['X']\n",
    "y = d['y']\n",
    "new_features = X@np.random.rand(9,3)\n",
    "X_new = np.concatenate((X, new_features), axis = 1)\n",
    "\n",
    "### (a)' ###\n",
    "n, p = np.shape(X_new)\n",
    "\n",
    "# error rate for regularized least squares\n",
    "error_SVD = np.zeros((8, 7))\n",
    "# SVD parameters to test\n",
    "k_vals = np.arange(9) + 1\n",
    "param_err_SVD = np.zeros(len(k_vals))\n",
    "\n",
    "subset_count = 8\n",
    "\n",
    "subsets = np.array_split(np.arange(n), 8)\n",
    "\n",
    "for i in range(subset_count):\n",
    "    X_hold_out = X_new[subsets[i]]\n",
    "    y_hold_out = y[subsets[i]]\n",
    "    \n",
    "    for idx, j in enumerate([j for j in range(subset_count) if j != i]):\n",
    "        X_train = np.concatenate([X_new[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        y_train = np.concatenate([y[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        \n",
    "        U, Sigma, VT = la.svd(X_train, full_matrices=False)\n",
    "\n",
    "        X_test = X_new[subsets[j]]\n",
    "        y_test = y[subsets[j]]\n",
    "\n",
    "        w_hat_lst = list()\n",
    "\n",
    "        for k_idx, k in enumerate(k_vals):\n",
    "            Sigma_plus = np.diag(1 / Sigma[:k])\n",
    "            VT_k = VT[:k, :]\n",
    "            U_k = U[:, :k]\n",
    "\n",
    "            w_hat = VT_k.T@Sigma_plus@U_k.T@y_train\n",
    "            w_hat_lst.append(w_hat)\n",
    "            y_hat = X_test@w_hat\n",
    "\n",
    "            param_err_SVD[k_idx] += np.sum(np.sign(y_hat) != y_test) / 16\n",
    "\n",
    "        # print(param_err_SVD)\n",
    "        k_best = np.argmin(param_err_SVD)\n",
    "        w_hat_best = w_hat_lst[k_best]\n",
    "\n",
    "        y_hat_hold_out = X_hold_out@w_hat_best\n",
    "        error_SVD[i][idx] = np.sum(np.sign(y_hat_hold_out) != y_hold_out) / 16\n",
    "print(f\"Error estimate: {np.mean(error_SVD)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error estimate: 0.05357142857142857\n"
     ]
    }
   ],
   "source": [
    "### (b)' ###\n",
    "# error rate for regularized least squares\n",
    "error_RLS = np.zeros((8, 7))\n",
    "# RLS parameters to test\n",
    "lambda_vals = np.array([0, 0.5, 1, 2, 4, 8, 16])\n",
    "param_err_RLS = np.zeros(len(lambda_vals))\n",
    "\n",
    "subset_count = 8\n",
    "\n",
    "subsets = np.array_split(np.arange(n), 8)\n",
    "\n",
    "for i in range(subset_count):\n",
    "    X_hold_out = X_new[subsets[i]]\n",
    "    y_hold_out = y[subsets[i]]\n",
    "    \n",
    "    for idx, j in enumerate([j for j in range(subset_count) if j != i]):\n",
    "        X_train = np.concatenate([X_new[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        y_train = np.concatenate([y[subsets[p]] for p in range(8) if p != i \n",
    "                                  and p != j])\n",
    "        \n",
    "        X_test = X_new[subsets[j]]\n",
    "        y_test = y[subsets[j]]\n",
    "\n",
    "        w_hat_lst = list()\n",
    "\n",
    "        for lmd_idx, lmd in enumerate(lambda_vals):\n",
    "            w_hat = la.inv(X_train.T@X_train + lmd*np.identity(p))@X_train.T@y_train\n",
    "            w_hat_lst.append(w_hat)\n",
    "            y_hat = X_test@w_hat\n",
    "\n",
    "            param_err_RLS[lmd_idx] += np.sum(np.sign(y_hat) != y_test) / 16\n",
    "\n",
    "        # print(param_err_SVD)\n",
    "        k_best = np.argmin(param_err_RLS)\n",
    "        w_hat_best = w_hat_lst[k_best]\n",
    "\n",
    "        y_hat_hold_out = X_hold_out@w_hat_best\n",
    "        error_RLS[i][idx] = np.sum(np.sign(y_hat_hold_out) != y_hold_out) / 16\n",
    "print(f\"Error estimate: {np.mean(error_RLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average error rate of Truncated SVD solution is 0.052455357142857144. <br>\n",
    "The average error rate of Regularized LS is 0.0546875. <br>\n",
    "<br>\n",
    "With new features,<br>\n",
    "Avg error rate of SVD_new is 0.049107142857142856 and <br>\n",
    "Avg error rate of RLS_new is 0.05357142857142857.<br>\n",
    "It seems that these new features are helpful for classification, but not significantly so. One possible explanation is that since the new features are generated as random linear combinations of the original features, they might not necessarily add any meaningful or informative variance to the model that would improve classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
